{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n\n#### Building A Machine Learning Models With Spark ML\n\n* Linear Regression : https://en.wikipedia.org/wiki/Linear_regression, https://www.youtube.com/watch?v=zPG4NjIkCjc\n* Decision trees    : http://www.r2d3.us/visual-intro-to-machine-learning-part-1/  (nice visualization for decision tree)\n* Random forest     : https://en.wikipedia.org/wiki/Random_forest\n* GB Decision Trees : https://en.wikipedia.org/wiki/Gradient_boosting\n* Clustering        : https://spark.apache.org/docs/2.3.0/ml-clustering.html, https://www.datascience.com/blog/k-means-clustering\n* Cross-validation  : https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n* Collaborative filtering: https://spark.apache.org/docs/2.3.0/mllib-collaborative-filtering.html, https://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/\n* Metrics:\n  * RMSE: https://en.wikipedia.org/wiki/Root-mean-square_deviation\n  * R2:   https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libs\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# general spark modules\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType #https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "# spark ml modules \n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training/testing data as Spark dataframe\n",
    "train = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/housing/train.csv')\n",
    "test  = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/housing/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 20 rows \n",
    "train.select('1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'KitchenAbvGr', 'BedroomAbvGr', 'TotRmsAbvGrd', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'PoolArea', 'SalePrice').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['GrLivArea', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'KitchenAbvGr', 'BedroomAbvGr', 'TotRmsAbvGrd', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'PoolArea', 'SalePrice']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "train = convertColumn(train, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets calculate some basic statistics about data\n",
    "train.select('1stFlrSF', '2ndFlrSF', 'SalePrice').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of target variable to improve stability of algorithms\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def log1p(v):\n",
    "      return np.log1p(v)\n",
    "\n",
    "train = train.withColumn('SalePriceLog', log1p(train.SalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select important columns\n",
    "columns.append('SalePriceLog')\n",
    "train = train.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `input_data` \n",
    "input_data = train.rdd.map(lambda x: (DenseVector(x[1:-2]), x[-1]))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df_for_ml = spark.createDataFrame(input_data, [\"features\", \"label\"])\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df_for_ml)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df_for_ml)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df = scaled_df.select('label', col('features_scaled').alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([0.8,  0.2],  seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing of Linear Regression\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10000, regParam=0.2, elasticNetParam=0.5)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = linearModel.summary\n",
    "print(\"RMSE train: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2   train: %f\" % trainingSummary.r2)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predicted)\n",
    "r2   = evaluator_r2.evaluate(predicted)\n",
    "\n",
    "print(\"\\nRMSE test: %f\" % rmse)\n",
    "print(\"r2   test: %f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializinf of Linear Regression\n",
    "lr = LinearRegression(labelCol=\"label\")\n",
    "\n",
    "# let's set desired parameters\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(lr.regParam,        [0.2, 0.3, 0.5,  0.7])\\\n",
    ".addGrid(lr.elasticNetParam, [0.2, 0.5,  0.7, 0.8])\\\n",
    ".addGrid(lr.maxIter,         [100, 1000, 5000, 10000])\\\n",
    ".build()\n",
    "\n",
    "# cross-validation settings\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=2018\n",
    "                         )  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = cvModel.bestModel.summary\n",
    "print(\"RMSE train: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2   train: %f\" % trainingSummary.r2)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = cvModel.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predicted)\n",
    "r2   = evaluator_r2.evaluate(predicted)\n",
    "\n",
    "print(\"\\nRMSE test: %f\" % rmse)\n",
    "print(\"r2   test: %f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision-Tree with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, [3,  10, 25])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = cvModel.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predicted)\n",
    "r2   = evaluator_r2.evaluate(predicted)\n",
    "\n",
    "print(\"\\nRMSE test: %f\" % rmse)\n",
    "print(\"r2   test: %f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = cvModel.bestModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = model.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predicted)\n",
    "r2   = evaluator_r2.evaluate(predicted)\n",
    "\n",
    "print(\"\\nRMSE test: %f\" % rmse)\n",
    "print(\"r2   test: %f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additonal outcome from trees is feature importances (can be used for feature selection)\n",
    "rf_model = model.stages[1]\n",
    "print(rf_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = model.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predicted)\n",
    "r2   = evaluator_r2.evaluate(predicted)\n",
    "\n",
    "print(\"\\nRMSE test: %f\" % rmse)\n",
    "print(\"r2   test: %f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rsandstroem.github.io/sparkkmeans.html\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(scaled_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(scaled_df)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model, scaled_df)"
   ]
  }
 ],
 "metadata": {
  "name": "Machine Learning in Spark",
  "notebookId": 1.600962361183146E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
