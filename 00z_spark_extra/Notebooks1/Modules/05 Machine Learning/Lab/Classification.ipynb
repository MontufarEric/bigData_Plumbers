{"cells":[{"cell_type":"code","source":["from __future__ import division\n\n# import necessary libs\nimport numpy  as np\nimport pandas as pd\n\n# general spark modules\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import lit\n\n# spark ml modules \nfrom pyspark.ml.linalg import DenseVector\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorIndexer\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\n# classification \nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import GBTClassifier\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nimport time\nimport itertools"],"metadata":{"collapsed":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["## Load Data"],"metadata":{}},{"cell_type":"code","source":["# load data as dataframe\ntrain_df = spark.read.csv('/FileStore/tables/titanic/train.csv', header=True)\ntest_df  = spark.read.csv('/FileStore/tables/titanic/test.csv',  header=True)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":["train_df.select('Parch').distinct().show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["train_df.show(10)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# prepare data for models\n\n# Write a custom function to convert the data type of DataFrame columns# Write \ndef convertColumn(df, names, newType):\n    for name in names: \n        df = df.withColumn(name, df[name].cast(newType))\n    return df "],"metadata":{"collapsed":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# cast numerical columns to float\n# numerical_cols = ['Age', 'SibSp', 'Parch', 'Fare']\nnumerical_cols = ['Fare', 'Age']\nlabel_col = ['Survived']\ntrain_df  = convertColumn(train_df,  numerical_cols + label_col, FloatType())\ntest_df   = convertColumn(test_df,   numerical_cols, FloatType())\n\n# fill missing values with 0\ntrain_df  = train_df.dropna()\ntest_df   = test_df.dropna()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["stages = []\ncategorical_cols = ['Embarked', 'Sex'] \n\nfor categorical_col in categorical_cols:\n    string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_index\", handleInvalid='error')\n    stages += [string_indexer]\n\nassembler_inputs = numerical_cols + [c + \"_index\" for c in categorical_cols]\nassembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n\nstages  += [assembler]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Model Selection"],"metadata":{}},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\npipelineModel = pipeline.fit(train_df)\n\n#  - transform() actually transforms the features.\ntrain = pipelineModel.transform(train_df)\ntest  = pipelineModel.transform(test_df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["train.show(10)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Split the data into train and test sets\ntrain_data, val_data = train.randomSplit([0.8,  0.2],  seed=1234)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Logistic regression"],"metadata":{}},{"cell_type":"code","source":["# define parameters\nregParam        = [0.1, 0.5, 2.0]\nelasticNetParam = [0.0,  0.5, 1.0]\nmaxIter         = [10, 50, 100]\nexperiments     = list(itertools.product(regParam, elasticNetParam, maxIter))\nprint len(experiments)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["for ind, experiment in enumerate(experiments):\n    regParam        = experiment[0]\n    elasticNetParam = experiment[1]\n    maxIter         = experiment[2]\n    \n    start_time = time.time()\n    print ind\n    print 'params: ', regParam, elasticNetParam, maxIter\n    \n    lr = LogisticRegression(labelCol=\"Survived\", \n                            featuresCol=\"features\", \n                            regParam=regParam,\n                            elasticNetParam=elasticNetParam,\n                            maxIter=maxIter\n                            )\n    \n    # Train model with Training Data\n    lrModel     = lr.fit(train_data)\n    \n    # Make predictions on validation data using the transform() method.\n    # LogisticRegression.transform() will only use the 'features' column.\n    predictions = lrModel.transform(val_data)\n    \n    # evaluate predictions\n    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='Survived')\n    auc       = evaluator.evaluate(predictions)\n    \n    print 'AUC: ', auc\n    print \"--- %s seconds ---\" % (time.time() - start_time)\n    print "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["predictions = lrModel.transform(test)\n# predictions.select('PassengerId', 'prediction').coalesce(1).write.csv('result.csv')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Decision Tree Classifier"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["# define parameters\nmaxDepth = [15, 30]\nmaxBins  = [10, 60, 80]\nexperiments     = list(itertools.product(maxDepth, maxBins))\nprint len(experiments)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["for ind, experiment in enumerate(experiments):\n    maxDepth = experiment[0]\n    maxBins  = experiment[1]\n\n    start_time = time.time()\n    print ind\n    print 'params: ', maxDepth, maxBins\n    \n    # Create initial Decision Tree Model\n    dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\", maxDepth=maxDepth, maxBins=maxBins)\n    \n    # Train model with Training Data\n    dtModel = dt.fit(train_data)\n    \n    # Make predictions on validation data using the transform() method.\n    # LogisticRegression.transform() will only use the 'features' column.\n    predictions = dtModel.transform(val_data)\n    \n    # evaluate predictions\n    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='Survived')\n    auc       = evaluator.evaluate(predictions)\n    print 'AUC: ', auc\n    print \"--- %s seconds ---\" % (time.time() - start_time)\n    print "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Random Forest"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["# define parameters\nnumTrees         = [5, 10, 50, 100]\nsubsamplingRate  = [0.8]\nmaxDepth         = [10, 15]\nexperiments      = list(itertools.product(numTrees, maxDepth, subsamplingRate))\nprint len(experiments)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["for ind, experiment in enumerate(experiments):\n    numTrees = experiment[0]\n    maxDepth = experiment[1]\n    subsamplingRate = experiment[2]\n\n    start_time = time.time()\n    print ind\n    print 'params: ', numTrees, maxDepth, subsamplingRate\n    \n    # Create an initial RandomForest model.\n    rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", \n                                numTrees=numTrees, \n                                maxDepth=maxDepth,\n                                subsamplingRate=subsamplingRate)\n    \n    # Train model with Training Data\n    rfModel = rf.fit(train_data)\n    \n    # Make predictions on validation data using the transform() method.\n    # LogisticRegression.transform() will only use the 'features' column.\n    predictions = rfModel.transform(val_data)\n    \n    # evaluate predictions\n    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='Survived')\n    auc       = evaluator.evaluate(predictions)\n    \n    print 'AUC: ', auc\n    print \"--- %s seconds ---\" % (time.time() - start_time)\n    print "],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.5","nbconvert_exporter":"python","file_extension":".py"},"name":"Classification","notebookId":3689821117038511},"nbformat":4,"nbformat_minor":0}
