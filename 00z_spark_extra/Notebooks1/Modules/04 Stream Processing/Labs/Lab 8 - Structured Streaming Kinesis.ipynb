{"cells":[{"cell_type":"markdown","source":["## Setting Up Kinesis Stream\nBefore starting this lab first you need to set up a kinesis stream in your amazon account and push bitcoin data from coindesk api. Do the following steps to set it up:\n1. Login to your AWS account and launch an EC2 instance. The smallest unit is fine as we will launch a very small task. If you are eligable for free tier you can also use it.\n2. Create a kinesis stream with the name \"bitcoin-exchange-rate\"\n3. Get an aws access key and secret key and make sure it has access to kinesis\n4. Configure your EC2's aws cli with these keys. To do so you can just type aws configure at the EC2 terminal and fill in the keys\n4. Launch the push_data_to_kinesis.py script (from this folder) as a process or via screen (so it always runs). Solve the required pip dependencies if required (i.e. install boto3)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import from_json, to_timestamp\nfrom pyspark.sql.functions import window\nfrom pyspark.sql.functions import avg"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Reading From Kinesis Stream"],"metadata":{}},{"cell_type":"markdown","source":["We will start our streaming application buy reading the data from Kinesis. Kinesis is a message queue, similar to Kafka, provided by AWS. You can read from Kinesis stream in the following way:"],"metadata":{}},{"cell_type":"code","source":["kinesisDF = spark \\\n  .readStream \\\n  .format(\"kinesis\") \\\n  .option(\"streamName\", \"bitcoin-exchange-rate\") \\\n  .option(\"initialPosition\", \"earliest\") \\\n  .option(\"region\", \"us-west-2\") \\\n  .option(\"awsAccessKey\", 'awsAccessKey') \\\n  .option(\"awsSecretKey\", 'awsSecretKey') \\\n  .load()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Its a good idea to clear up old files, streaming application produces a lot of temp files, and there is a limit to how many temp files you can have for a free Databricks account."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm('dbfs:/SOME_CHECKPOINT_DIRECTORY/', True)\ndbutils.fs.rm(('dbfs:/tmp/'), True)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["We will enforce a schema as it is more efficient, if we leave it blank Spark can figure out the Schema as well!"],"metadata":{}},{"cell_type":"code","source":["pythonSchema = StructType().add(\"timestamp\", StringType()).add(\"EUR\", FloatType()).add(\"USD\", FloatType()).add (\"GBP\", FloatType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now we will read from the stream into our streaming dataframe!"],"metadata":{}},{"cell_type":"code","source":["bitcoinDF = kinesisDF.selectExpr(\"cast (data as STRING) jsonData\").select(from_json(\"jsonData\", pythonSchema).alias(\"bitcoin\")).select(\"bitcoin.*\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We will also convert the timestamp column to the timestamp type so we can query with datetime object in Python"],"metadata":{}},{"cell_type":"code","source":["bitcoinDF = bitcoinDF.withColumn('timestamp', to_timestamp(bitcoinDF.timestamp, \"yyyy-MM-dd HH:mm:ss\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(bitcoinDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Quering! \nNow you can use all the things you learnt previously from Spark SQL! For example, you can groupBy certain attribute and aggregate, filter, or select as you wish! <br/>\nWe haven't been introduced to the concept of windowing, which we will briefly zoom in now."],"metadata":{}},{"cell_type":"markdown","source":["A window function can also be applied to to Bucketize rows into one or more time windows given a timestamp specifying column. For that we will use window groupBy function (pyspark.sql.functions.window) <br/>\nyou can call the window groupby function in the following way: __window(timeColumn, windowDuration, slideDuration=None, startTime=None)__. The definition of slide interval and window interval are as follows:\n* Window Duration: how far back in time the windowed transformation goes\n* Slide Duration: how often a windowed intransformation is computed"],"metadata":{}},{"cell_type":"code","source":["windowedCounts = bitcoinDF.groupBy(window(bitcoinDF.timestamp, \"10 minutes\", \"5 minutes\").alias('time_window')).agg(avg(bitcoinDF.EUR).alias('window_avg_euro_rate'))\ndisplay(windowedCounts)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# to read the stream from memory we will set up a table in our memory called bitcoin_window\nquery = windowedCounts.writeStream.format(\"memory\").queryName(\"bitcoin_window\").outputMode(\"complete\").start()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# you can then take your stream to a dataframe using SQL queries in the following way\ndf = spark.sql('select time_window.start, time_window.end, window_avg_euro_rate from bitcoin_window')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# we can query on top of live data for average euro rate for the last hour\nfrom datetime import datetime, timedelta\nfrom pyspark.sql.functions import avg\ntimedelta_ten_mins = datetime.now() - timedelta(minutes=60)\nlast_hour_rate_query = df.filter(df.start > timedelta_ten_mins).select('window_avg_euro_rate').agg(avg('window_avg_euro_rate').alias('rate')).collect()\nprint(last_hour_rate_query[0].rate)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Excercise\nTry writing a query to window maximum bitcoin rate per ten minutes for USD, EUR, and GBP. Show them all in line chart."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Challenge\nWrite a smart algorithm to trade bitcoin automatically. Start with a hypothetical amount (ex. ten bitcoin) and trade it between currencies and see if you can automatically increase your net worth by trading."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Spark Lab 7","notebookId":1659965816319766},"nbformat":4,"nbformat_minor":0}
